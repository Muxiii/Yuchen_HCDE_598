# Yuchen_HCDE_598

# Exploration1
For exploration1 task, I used Prototype 1 and completed both <hard> and <medium> level tasks.

Explore 1.2 (Medium):
Many LLM services include mechanisms to cover their operational costs, often by charging users based on usage. In other words, API users are billed according to the amount of text generated. The response returned by the make_chat_request() function includes information about this usage. Modify the make_chat_request() function so that it returns the usage data for each request. Then, update the loop in main() to accumulate the total usage over the course of a chat session. Once the session ends, print out the total usage to provide a clear summary of token consumption.

Explore 1.3 (Hard):
The API specification allows us to send parameters in the request body to influence how the LLM generates text. These parameters become part of the chat_context sent with each request. Two important parameters to explore are temperature and max_completion_tokens. Modify the code to include two new constants named OAI_MODEL_TEMPERATURE and OAI_MODEL_MAX_TOKENS. Then, update the new_chat_context() function to incorporate these parameters and assign them their respective constant values. Consult the OpenAPI documentation to observe how different values for temperature and max_completion_tokens impact the responses generated by the model.
